<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
<style>
/* CSS for Markstat 2.0 using Pandoc 2.0 */
/* Adapted from original */

html{width:70%; margin:2rem auto;background-color: lightgray;}
body{background-color: white; padding: 6rem 9rem;}
body, table {font-family: Helvetica, Arial, Sans-serif; font-size: 14px;}
h1, h2, h3, h4 {font-weight: normal; color: #000}
h1 {font-size: 200%; font-weight: bold;}
h2 {font-size: 150%; font-weight: bold; margin-top: 4rem;}
h3 {font-size: 120%; font-weight: bold; margin-top: 3rem;}
h4 {font-size: 100%; font-weight:bold}
img, img.center {display:block; margin-left:auto; margin-right:auto;margin-top:2rem; margin-bottom:2rem}
.small{font-size:8pt;}
a {color: black;}
a:visited {color: #808080;}
a.plain {text-decoration:none;}
a.plain:hover {text-decoration:underline;}
.em {font-weight:bold;}
pre, code {font-family: "lucida console", monospace; margin: 2rem auto;}
pre.stata {font-size:13px; line-height:13px;}
pre {padding:8px; border:1px solid #c0c0c0; border-radius:8px; background-color:#fdfdfd;}
code {color:#000; background-color:#efefef;}
pre code { color:black; background-color:white}
hr {margin: 2rem auto; width: 25rem;}
	


/* Added for Pandoc */
figure > img, div.figure > img {display:block; margin:auto}
figcaption, p.caption {text-align:center; font-weight:bold; color:#000;}
h1.title {text-align:center; margin-bottom:0}
p.author, h2.author {font-style:italic; text-align:center;margin-top:4px;margin-bottom:0}
p.date, h3.date {text-align:center;margin-top:4px; margin-bottom:0}
p+img {text-align:center;}

/* Tables*/
table { margin:auto; border-collapse:collapse; }
table caption { margin-bottom:1ex;}
th, td { padding:4px 6px;}
thead tr:first-child th {border-top:1px solid black; padding-top:6px}
thead tr:last-child  th {padding-bottom:6px}
tbody tr:first-child td {border-top:1px solid black; padding-top:6px}
tbody tr:last-child  td {padding-bottom:6px;}
table tbody:last-child tr:last-child td {border-bottom:1px solid black;}
</style>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Dale Barnhart, Robert Brennan, Simo Goshev" />
  <title>Introduction to Missing Data and Imputation (Session 1)</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Introduction to Missing Data and Imputation (Session 1)</h1>
<p class="author">Dale Barnhart, Robert Brennan, Simo Goshev</p>
<p class="date">9 Oct 2018 , v0.01</p>
</header>

<p><em>The following borrows heavily from Simo’s tutorial on missing data</em></p>
<h2 id="missing-data">Missing data</h2>
<h3 id="what-is-missing-data">What is missing data?</h3>
<p>Generally we want data to be complete – that is we wish that all individuals we have sampled agree to be interviewed and subsequently provide valid responses to all applicable questions in the questionnaire.</p>
<p>However, more often than not data are incomplete. For example, sampled individuals may fall out of reach (i.e. move or simply ignore us), or decline to be interviewed. If we retained this type of respondents in the sample, our dataset would have observations that are subject to <em>unit non-response</em>. That is, we would not have any response data at all for these subjects.</p>
<p>Subjects can also decline to answer certain questions or simply skip questions. This is known as <em>item non-response</em> and it manifests as a “gappy” pattern in the dataset.</p>
<p>Unit and item non-response are the patterns of missingness commonly present in data used in social science research, and missing data, as you can tell already, is simply the full or partial absence of valid values or measurements for one or more subjects for one or more variables.</p>
<h3 id="why-missing-data-may-be-a-problem">Why missing data may be a problem?</h3>
<p>We have collected data for a sample of individuals that include person, time and treatment identifiers as well as demographic variables and responses to four psycho-social scales: <code>s1</code>, <code>s2</code>, <code>s3</code> and <code>s4</code>.</p>
<p>We know that there is some missingness in our data but before we proceed with analyzing the data, we want to know more about how serious missigness is and whether the amount of missingness may create problems for our analysis.</p>
<p>We read the data in look at the first 12 responses (4 subjects observed 3 times) for <code>s1</code>.</p>
<pre class='stata'>. simdata 200 3

. list id time s1_i* s2_i* in 1/30

     ┌───────────────────────────────────────────────────┐
     │ id   time   s1_i1   s1_i2   s1_i3   s2_i1   s2_i2 │
     ├───────────────────────────────────────────────────┤
  1. │  1      1       1       1       0       2       2 │
  2. │  1      2       1       0       0       3       1 │
  3. │  1      3       1       0       1       3       2 │
  4. │  2      1       1       2       1       5       . │
  5. │  2      2       2       2       1       2       2 │
     ├───────────────────────────────────────────────────┤
  6. │  2      3       2       1       2       2       5 │
  7. │  3      1       0       0       0       .       1 │
  8. │  3      2       0       0       0       0       0 │
  9. │  3      3       0       0       0       0       3 │
 10. │  4      1       0       0       0       0       0 │
     ├───────────────────────────────────────────────────┤
 11. │  4      2       0       0       0       2       2 │
 12. │  4      3       0       0       0       3       0 │
 13. │  5      1       0       1       1       3       2 │
 14. │  5      2       0       0       0       4       2 │
 15. │  5      3       0       1       1       2       4 │
     ├───────────────────────────────────────────────────┤
 16. │  6      1       0       0       0       1       0 │
 17. │  6      2       0       0       0       2       0 │
 18. │  6      3       .       0       1       2       0 │
 19. │  7      1       2       1       2       4       5 │
 20. │  7      2       1       1       2       1       1 │
     ├───────────────────────────────────────────────────┤
 21. │  7      3       1       1       .       4       2 │
 22. │  8      1       0       0       0       0       0 │
 23. │  8      2       0       1       0       1       1 │
 24. │  8      3       0       0       0       2       0 │
 25. │  9      1       1       1       2       0       3 │
     ├───────────────────────────────────────────────────┤
 26. │  9      2       1       2       1       4       3 │
 27. │  9      3       1       2       1       1       4 │
 28. │ 10      1       0       1       1       4       1 │
 29. │ 10      2       1       0       1       4       2 │
 30. │ 10      3       0       1       0       1       4 │
     └───────────────────────────────────────────────────┘
</pre>
<p>Are the rest of the scales look similar? Let’s use command <code>misstable</code> to explore the missingness further.</p>
<p>We first look at a summary table of the missingness</p>
<pre class='stata'>. misstable summarize, gen(fl)
                                                               Obs&lt;.
                                                ┌──────────────────────────────
               │                                │ Unique
      Variable │     Obs=.     Obs>.     Obs&lt;.  │ values        Min         Max
  ─────────────┼────────────────────────────────┼──────────────────────────────
         s1_i1 │        30                 570  │      5          0           4
         s1_i2 │        29                 571  │      5          0           4
         s1_i3 │        35                 565  │      5          0           4
         s2_i1 │        41                 559  │      8          0           7
         s2_i2 │        34                 566  │      7          0           6
         s3_i1 │        27                 573  │      2          0           1
         s3_i2 │        26                 574  │      2          0           1
         s3_i3 │        28                 572  │      2          0           1
         s3_i4 │        36                 564  │      2          0           1
         s4_i1 │        32                 568  │   >500          0    51.90791
         s4_i2 │        34                 566  │   >500          0    52.10009
         s4_i3 │        23                 577  │   >500          0     52.0597
         s4_i4 │        35                 565  │   >500          0    51.07303
  ─────────────┴────────────────────────────────┴──────────────────────────────
</pre>
<p>Next, we explore the pattern of missingness</p>
<pre class='stata'>. misstable pattern, bypat

                    Missing-value patterns
                      (1 means complete)

              │   Pattern
    Percent   │  1  2  3  4    5  6  7  8    9 10 11 12   13
  ────────────┼──────────────────────────────────────────────
       49%    │  1  1  1  1    1  1  1  1    1  1  1  1    1
              │
  1:          │
        4     │  1  1  1  1    1  1  1  1    1  1  1  1    0
        4     │  1  1  1  1    1  1  1  1    1  1  1  0    1
        3     │  1  1  1  1    1  1  1  1    1  1  0  1    1
        3     │  1  1  1  1    1  1  1  1    1  0  1  1    1
        3     │  1  1  1  1    1  1  1  1    0  1  1  1    1
        3     │  1  1  1  1    1  1  1  0    1  1  1  1    1
        2     │  1  1  1  1    1  1  0  1    1  1  1  1    1
        3     │  1  1  1  1    1  0  1  1    1  1  1  1    1
        2     │  1  1  1  1    0  1  1  1    1  1  1  1    1
        3     │  1  1  1  0    1  1  1  1    1  1  1  1    1
        2     │  1  1  0  1    1  1  1  1    1  1  1  1    1
        3     │  1  0  1  1    1  1  1  1    1  1  1  1    1
        2     │  0  1  1  1    1  1  1  1    1  1  1  1    1
  2:          │
       &lt;1     │  1  1  1  1    1  1  1  1    1  1  0  1    0
       &lt;1     │  1  1  1  1    1  1  1  1    1  0  1  1    0
       &lt;1     │  1  1  1  1    1  1  1  1    1  0  1  0    1
       &lt;1     │  1  1  1  1    1  1  1  1    1  0  0  1    1
       &lt;1     │  1  1  1  1    1  1  1  1    0  1  1  1    0
       &lt;1     │  1  1  1  1    1  1  1  1    0  1  1  0    1
       &lt;1     │  1  1  1  1    1  1  1  1    0  0  1  1    1
       &lt;1     │  1  1  1  1    1  1  1  0    1  1  1  1    0
       &lt;1     │  1  1  1  1    1  1  1  0    1  1  1  0    1
       &lt;1     │  1  1  1  1    1  1  1  0    1  1  0  1    1
       &lt;1     │  1  1  1  1    1  1  1  0    1  0  1  1    1
       &lt;1     │  1  1  1  1    1  1  1  0    0  1  1  1    1
       &lt;1     │  1  1  1  1    1  1  0  1    1  1  1  1    0
       &lt;1     │  1  1  1  1    1  1  0  1    1  1  1  0    1
       &lt;1     │  1  1  1  1    1  1  0  1    1  1  0  1    1
       &lt;1     │  1  1  1  1    1  1  0  1    1  0  1  1    1
       &lt;1     │  1  1  1  1    1  0  1  1    1  1  1  1    0
       &lt;1     │  1  1  1  1    1  0  1  1    1  1  1  0    1
       &lt;1     │  1  1  1  1    1  0  1  1    1  1  0  1    1
       &lt;1     │  1  1  1  1    1  0  0  1    1  1  1  1    1
       &lt;1     │  1  1  1  1    0  1  1  1    1  1  1  1    0
       &lt;1     │  1  1  1  1    0  1  1  1    1  1  1  0    1
       &lt;1     │  1  1  1  1    0  1  1  1    1  1  0  1    1
       &lt;1     │  1  1  1  1    0  1  1  1    0  1  1  1    1
       &lt;1     │  1  1  1  1    0  1  0  1    1  1  1  1    1
       &lt;1     │  1  1  1  1    0  0  1  1    1  1  1  1    1
       &lt;1     │  1  1  1  0    1  1  1  1    1  1  1  1    0
       &lt;1     │  1  1  1  0    1  1  1  1    1  1  1  0    1
       &lt;1     │  1  1  1  0    1  1  1  1    1  1  0  1    1
       &lt;1     │  1  1  1  0    1  1  1  1    0  1  1  1    1
       &lt;1     │  1  1  1  0    1  1  0  1    1  1  1  1    1
       &lt;1     │  1  1  1  0    1  0  1  1    1  1  1  1    1
       &lt;1     │  1  1  0  1    1  1  1  1    1  1  1  1    0
       &lt;1     │  1  1  0  1    1  1  1  1    1  1  1  0    1
       &lt;1     │  1  1  0  1    1  1  1  1    0  1  1  1    1
       &lt;1     │  1  1  0  1    1  1  1  0    1  1  1  1    1
       &lt;1     │  1  1  0  1    1  1  0  1    1  1  1  1    1
       &lt;1     │  1  1  0  1    1  0  1  1    1  1  1  1    1
       &lt;1     │  1  0  1  1    1  1  1  1    1  1  1  1    0
       &lt;1     │  1  0  1  1    1  1  1  1    1  0  1  1    1
       &lt;1     │  1  0  1  1    1  1  0  1    1  1  1  1    1
       &lt;1     │  1  0  1  0    1  1  1  1    1  1  1  1    1
       &lt;1     │  1  0  0  1    1  1  1  1    1  1  1  1    1
       &lt;1     │  0  1  1  1    1  1  1  1    1  1  1  1    0
       &lt;1     │  0  1  1  1    1  1  1  1    1  0  1  1    1
       &lt;1     │  0  1  1  1    1  1  1  1    0  1  1  1    1
       &lt;1     │  0  1  1  1    1  1  1  0    1  1  1  1    1
       &lt;1     │  0  1  1  1    1  1  0  1    1  1  1  1    1
       &lt;1     │  0  1  1  1    0  1  1  1    1  1  1  1    1
  3:          │
       &lt;1     │  1  1  1  1    1  1  0  0    1  1  1  0    1
       &lt;1     │  1  1  1  1    1  0  1  1    1  1  0  1    0
       &lt;1     │  1  1  1  1    1  0  0  1    1  1  0  1    1
       &lt;1     │  1  1  1  0    1  1  1  0    1  0  1  1    1
       &lt;1     │  1  1  0  1    0  1  1  0    1  1  1  1    1
       &lt;1     │  1  0  1  1    1  1  1  1    1  1  0  1    0
       &lt;1     │  1  0  1  0    1  1  1  0    1  1  1  1    1
  4:          │
       &lt;1     │  1  1  1  1    1  0  0  0    1  0  1  1    1
       &lt;1     │  0  1  1  1    1  1  0  1    1  1  0  0    1
       &lt;1     │  0  0  1  0    1  1  0  1    1  1  1  1    1
  ────────────┼──────────────────────────────────────────────
      100%    │

  Variables are  (1) s4_i3  (2) s3_i2  (3) s3_i1  (4) s3_i3  (5) s1_i2  (6) s1_i1  (7) s4_i1
                 (8) s4_i2  (9) s2_i2  (10) s4_i4  (11) s1_i3  (12) s3_i4  (13) s2_i1
</pre>

<p>It appears that some scales have more missingness than others. The proportion of complete responses is 49%. The proportion of subject who responded to all questions in all waves is 119%.</p>
<h2 id="missing-data-mechanisms">Missing data mechanisms</h2>
<p>As we will see shortly, there are three distinct missing-data mechanisms defined in the literature. To understand better these mechanisms and how they defer from each other, we first need to define two terms: <em>observed</em> and <em>unobserved</em> data.</p>
<p><strong>Observed data</strong> are the values or measurements that a researcher collects. In our dataset, these would be the recorded values (except for the missing values) for all variables and items.</p>
<p><strong>Unobserved data</strong> are data that researchers fails to measure.</p>
<h3 id="missing-completely-at-random-mcar">Missing completely at random (MCAR)</h3>
<p>When we say that data are missing completely at random (MCAR), we mean that the missing-data mechanism is independent of both the observed and unobserved data. We can think of MCAR as a mechanism that introduces missigness to complete data by changing, on an entirely independent and random basis, the values of some variables to missing.</p>
<p>In theory, we would not expect incomplete records (or observations that contain missing values for some variables) to differ systematically from complete records.</p>
<p>When working with surveys, very rarely can researchers claim/assume that missingness is MCAR. The reason is that usually there are underlying causes for subjects not to respond, causes which may or may not have been observed by the researcher (as part of the study).</p>
<h3 id="missing-at-random-mar">Missing at random (MAR)</h3>
<p>When we say that data are missing at random (MAR), we mean that the missing-data mechanism is dependent on the observed data but is independent of the unobserved data. We can think of this mechanism as one that introduces missigness to complete data by changing the values of some variables to missing based on the observed values of other, observed variables belonging to the same record (subject).</p>
<p>In theory, we expect to see difference between complete and MAR data, specifically in the variables affected by missingness.</p>
<p>Since under MAR the mechanism of the missing data is dependent on the observed data only, we are able to both test for this missingness mechanism and address the missingness methodologically.</p>
<p>When working with surveys, MAR is the type of mechanism that is commonly assumed. There are standard methods for handling it, some of which we will discuss shortly.</p>
<h3 id="missing-not-at-random-mnar">Missing not at random (MNAR)</h3>
<p>When we say that data are missing not at random (MNAR), we mean that the missing-data mechanism is dependent on the unobserved data. We can think of this mechanism as one that introduces missigness to complete data by changing the values of some variables to missing based on either the unobserved values of the same variable or the values of other, unobserved variables belonging to the same record.</p>
<p>With MNAR, we expect incomplete and complete cases to differ systematically. There are several classes of methods that can handle MNAR data of which sample selection models (with the Heckman selection model being the flagship model) is the most popular class in social science.</p>
<h2 id="strategies-for-handling-anticipated-missingness">Strategies for Handling (Anticipated) Missingness</h2>
<p>We have seen so far that missingness may impact the estimates of quantities of interest. Data collectors and applied researchers utilize a variety of strategies and methods to minimize or mitigate the impact of missingness.</p>
<h3 id="prior-to-data-collection">Prior to data collection</h3>
<p><em>“Design trumps model”</em> the old saying goes, and it could not be more relevant to addressing missingness. To minimize missingness, the data collector has to maximize effort in designing a solid data collection plan (containing various provisions and fallbacks) and executing it with a high level of fidelity.</p>
<h4 id="oversampling">Oversampling</h4>
<p>At the stage of survey sampling design, it is almost always advisable to provision for oversampling. Oversampling means that we will sample more subjects that we think we would need. How many more subject you may ask? Well, some survey agencies sample 5% more, others sample 10% more. It all depends on the amount of missingness or non-response one might expect (and also of course on cost).</p>
<p>If in doubt, it is always recommended to ask a survey design specialist and/or someone very familiar with the population that will give rise to the sample.</p>
<h4 id="pilot-studies">Pilot studies</h4>
<p>Researchers should always run pilot studies. Pilots can help in several imporant ways:</p>
<ul>
<li><p>Validate the survey instrument (questionnaire that is)</p></li>
<li><p>Validate the rules and procedures of data collection, management and storage</p></li>
<li><p>Gain preliminary inderstanding of expected unit and item non-response</p></li>
<li><p>Test strategies for minimizing non-response (i.e. incentivisation)</p></li>
</ul>
<h4 id="rigor-during-data-collection">Rigor during data collection</h4>
<p>Once a respondent agrees to participate, we want to help them provide a complete response. This may mean working around respondent’s constraints or employing some form of incentivisation. We always want to consult with the Institutional Review Board’s (IRB) about acceptable forms of incentivising response.</p>
<p>We also want to avoid strategies that will create perverse incentives as they are likely to ruin our study!</p>
<h3 id="after-data-collection">After data collection</h3>
<p>Despite our best effort during the planning and collection stages, our data may still be subject to missingness. What can we do ex-post?</p>
<h4 id="weighting">Weighting</h4>
<p>Weighting, or adjusting the relative contribution of respondents’ data to estimated quantities of interest, is a popular way of correcting for unit and item non-response.</p>
<p>A commonly used weighting techniques (in the past) involves calculating probability of reponse weights for each subject in the sample and then using these weights in subsequent analyses. This technique is known as <em>inverse probability weighting</em> (IPW). IPW are commonly implemented in the following manner: we would predict the probability of a complete response for every subject as a function of their observables and then use the inverse of the predicted probabilities as a probability weight in our complete-case analysis.</p>
<p><em>What does IPW really do for us?</em></p>
<p>Suppose the likelihood of a complete response of a person with specific charasterics is 1. The weight that we will assign to this individual would therefore be 1 which in effect means that they will be counted once in the sample – they will only represent themselves as everyone else with the same characteristics would have provided a complete response as well.</p>
<p>Now, suppose the likelihood of a complete response of a person with a different set of characteristics is 0.5. This means that we would expect only half of the people with the same charastestics to provide a complete response. Therefore everyone who actually responded to all questions will be given a weight of 2, or will be counted twice: once for themselves and once for a respondent with same characteristics who did not respond.</p>
<p>With likelihood of 0.1, everyone with same characteristics who responds will represent themselves and 9 other non-responders like them.</p>
<p><em>Two-phase estimation for missing data</em></p>
<p>This is a method that assumes the following process governing missingness:</p>
<p>Phase 1: Original sample selected and variables without missing values are measured<br />
Phase 2: A subset of respondents is selected and the remaining variable are observed</p>
<p>Obvious limitations of this method? It is only useful when we can divide the observations into complete and incomplete</p>
<h4 id="imputation">Imputation</h4>
<p>Imputation methods are an alternative. Imputation generally refers to the process of filling in the “gaps” or missingness in our dataset with valid values in a principled manner. Imputation methods generally fall into two categories: methods for <em>single imputation</em> and methods for <em>multiple imputation</em>. <br> <br></p>
<p><em>Single imputation</em></p>
<p>Methods that fall under the umbrella of <em>single imputation</em> include hot- and cold-deck imputation, mean substitution and regression.</p>
<p><em>Hot-deck imputation</em></p>
<p>The method involves replacing missing values with values from a random <em>similar</em> observation in the same dataset.</p>
<p><em>Cold-desk imputation</em></p>
<p>The method involves replacing missing values with values from a random <em>similar</em> observation from a different dataset.</p>
<p><em>Mean substitution</em></p>
<p>The method involves replacing missing values of a variable with the <em>mean</em> value of the variable computed over non-missing observations.</p>
<p><em>Regression</em></p>
<p>The method involves estimating a regression equation from all complete cases and then replacing the missing values with the respective predicted values from the regression. <br></p>
<p>There are many other single imputation techniques but ultimately all of them are subject to one common limitation. Could you name it? <br> <br></p>
<p><em>Multiple imputation (MI)</em></p>
<p>Multiple imputation, a simulation-based technique proposed by Donald Rubin, overcomes the problems of variability that single imputation methods fail to address. (Yes, single imputation methods do not account for the uncertainty in the imputed values; that is their common limitation.) In a series of articles in the 1970’s and 1980’s, culminating with his seminal book “Multiple Imputation for Nonresponse in Surveys” published in 1987, Rubin developed the theoretical basis of MI and proposed the following general algorithm for its implementation:</p>
<ol type="1">
<li>Impute the missing values <span class="math inline">\(M\)</span> times, thus generating <span class="math inline">\(M\)</span> complete datasets</li>
<li>Estimate the quantities of interest from every <span class="math inline">\(m\)</span> dataset in <span class="math inline">\(M\)</span></li>
<li>Combine the estimates from every dataset in <span class="math inline">\(M\)</span> into a final set of estimates that can be used for inference.</li>
</ol>
<p>The following formulas for aggregation used in Step 3 are known as the <em>Rubin’s rules</em>:</p>
<p><span class="math display">\[
    \hat{\beta} = \frac{1}{m}\sum_{m=1}^{M}\hat{\beta}_{m}
\]</span> <span class="math display">\[
    V_{\beta} = W + (1 + \frac{1}{m})B
\]</span></p>
<p>where <span class="math display">\[
    W=\frac{1}{m}\sum_{m=1}^{M}s_{m}^{2}
\]</span> <span class="math display">\[
    B=\frac{1}{m-1}\sum_{m=1}^{M}(\hat{\beta}_{m} - \hat{\beta})^{2}
\]</span></p>
<p>Nowadays, multiple imputation is the recommended technique for handling missing data. <br> <br></p>
<p><em>Full-likelihood methods (FLM)</em></p>
<p>FLM are based on the joint distribution of the variables affected by missingness together with the outcome of the regression model we wish to estimate. We specify a likelihood function which we then estimate to retrieve the most likely regression paratemers. A main limitation of this method is that all variables need to be continuous and the regression model has to be linear. <br> <br></p>
<p><em>Doubly robust methods (DRM)</em></p>
<p>While with IPW weighting methods we model the probability of complete cases and mulitple imputation models the distribution of missing variables, DRM combine the two approaches. Generally, DRM use two models: one predicts the missing values and the other predicts the missing probabilities (which are used as weights). There are both parametric and semi-parametric implemetations of DRM but they have remained somewhat on the sidelines in social science research.</p>
<p><br> <br></p>
<h2 id="multiple-imputation">Multiple Imputation</h2>
<p>Multiple imputation methods fall under two broad categories: <em>univariate</em> MI methods and <em>multivariate</em> MI methods. We use univariate methods for multiple imputation if missingness is MCAR or MAR and we wish to impute the missing values of a single variable. In comparison, multivariate multiple imputation (MMI) methods are helpful if (missingness is MCAR or MAR and) we wish to impute multiple variables with missing values simultaneously while (hopefully!) preserving the relationships among them.</p>
<p>This ability to allow for relationships among the variables (missing or non-missing) in a dataset is what makes the MMI methods so popular in social science research. We will focus on three distinct flavors of MMI in order of specification flexibility they provide: imputation using multivariate normal regression, imputation in monotone data, and chained imputation.</p>
<h3 id="imputation-using-multivariate-normal-regression">Imputation using multivariate normal regression</h3>
<p>This method of imputation can be used when imputing one or more continuous variables. It uses multivariate normal regression to model the mean function and Markov Chain Monte Carlo to impute the missing values.</p>
<hr />
<p><em>Oh wait, what is a Markov chain?</em></p>
<p>A Markov chain is a mathematical model of a stochastic system. It is defined by a set of states and a set of transition probabilities for traversing among these states. Intuitively, we can think of Markov chains as random walks on graphs. <a href="http://setosa.io/ev/markov-chains/"><em>This</em></a> website has several interactive examples of Markov chains that can help us understand them better.</p>
<p><em>And what is a Markov Chain Monte Carlo (MCMC)?</em></p>
<p>MCMC is a class of algorithms that allow us to sample from complex probability distributions. The Monte Carlo part of MCMC refers to the fact that we are simulating a distribution and the Markov Chain part refers to the fact that we are using a Markov chain to do the sampling.</p>
<p><em>And how does MCMC impute the missing values?</em></p>
<p>Under specific conditions, a Markov chain will have a <em>stationary distribution</em>. This means that for a sufficiently long random walk, the set of transition probabilities will converge to some fixed quantities and will not depend on the starting state of the walk.</p>
<p>Now, if we set the stationary distribution of the Markov chain to the distribution that we wish to sample from (i.e. the <em>target distribution</em>), for a sufficiently long random walk, the Markov chain will reproduce empirically this distribution and will help us sample from it. In imputation problems (based on regression), we normally pick a target distribution and a mean function and let the developers of statistical software choose the specific MCMC algorithm that would be most efficient for sampling from this distribution.</p>
<hr />
<p>The imputation algorithm has two steps: an <em>imputation</em> step and a <em>posterior</em> step. In the imputation step, the missing values are replaced by draws from our target distribution – a multivariate normal distribution, conditional on the observed data and current values of the model parameters. In the posterior step, new values of the model parameters and variance are drawn from their respective distributions, conditional on the observed data and the imputed values from the previous imputation step. This procedure is repeated until a pre-specified number of iterations is reached.</p>
<p>An obvious disadvantage of this method is that it only works for imputing continuous variables (that need to follow a normal distribution). This is a pretty significant limitation considering the variety of variable types that are present in social science data.</p>
<p>Let us apply this method to three of the items of scale <code>s1</code>. First, we reshape the dataset to wide form.</p>

<p>Then we declare imputed variables and collect all items of a scale in a respective list.</p>
<pre class='stata'>. mi set flong

. mi register imputed s1_i* s2_i* s3_i* s4_i*
(181 m=0 obs. now marked as incomplete)
</pre>

<p>Finally, we impute the missing values of item 1 in scale 1.</p>
<pre class='stata'>. mi impute mvn `s1' = x1* i.x2* x3* y*, add(5)  saveptrace(mytrace, replace)
note: x12 omitted because of collinearity
note: x13 omitted because of collinearity

Performing EM optimization:
note: x12 omitted because of collinearity
note: x13 omitted because of collinearity
  observed log likelihood =  15.724247 at iteration 9

Performing MCMC data augmentation ... 

Multivariate imputation                     Imputations =        5
Multivariate normal regression                    added =        5
Imputed: m=1 through m=5                        updated =        0

Prior: uniform                               Iterations =      500
                                                burn-in =      100
                                                between =      100

───────────────────┬──────────────────────────────────────────────
                   │               Observations per m             
                   ├───────────────────────────────────┬──────────
          Variable │   Complete   Incomplete   Imputed │     Total
───────────────────┼───────────────────────────────────┼──────────
         s1_i1_tp1 │        188           12        12 │       200
         s1_i1_tp2 │        192            8         8 │       200
         s1_i1_tp3 │        190           10        10 │       200
───────────────────┴───────────────────────────────────┴──────────
(complete + incomplete = total; imputed is the minimum across m
 of the number of filled-in observations.)
</pre>
<p>The next step is to check convergence. We use command <code>mi ptrace</code> to retrieve the iteration logs and plot the coefficient estimates of age for each of the three imputed variables.</p>

<p><img src="mvn_s1_i1.png" /></p>
<p>Autocorrelation plot of the same thing:</p>

<p><img src="mvn_s1_i1_ac.png" /></p>
<p>As we can see, there is some correlation between iterations in the simulated values of the parameters on age.</p>
<p>Two parameters of importance: <code>burnin</code> and <code>burnbetween</code>.</p>
<h3 id="imputation-in-monotone-data">Imputation in monotone data</h3>
<p>To understand this method, first we need to define the term “monotone missing pattern”. Data exhibit a monotone missing pattern if missingness in variable <span class="math inline">\(Y_{j}\)</span> for subject <em>i</em> means that variables <span class="math inline">\(Y_{k}\)</span>, where <span class="math inline">\(k &gt; j\)</span> for subject <em>i</em> are also missing.</p>
<p>The presence of a monotone missing pattern in data simplifies the imputation task by reducing it to a sequence of independent univariate (possibly conditional) imputation tasks.</p>
<p>The imputation algorithm has the following general logic:</p>
<p><span class="math display">\[\begin{equation}
\begin{array}{clc}

X_{1}^{*} \leftarrow Z \\
X_{2}^{*} \leftarrow X_{1}^{*}, Z \\
X_{3}^{*} \leftarrow X_{1}^{*}, X_{2}^{*}, Z

\end{array}
\end{equation}\]</span></p>
<p>The advantages of this method are that not only can we simplify the imputation task but we can also specify univariate models that correspond to the types of variables we are imputing. This is a rather big improvement over the method of multivariate normal regression. The disadvantage is that it is quite uncommon for data to have a monotone missing pattern.</p>
<p>Stata’s command is <code>mi impute monotone</code>.</p>
<h3 id="multiple-imputation-using-chained-equations">Multiple imputation using chained equations</h3>
<p>Multiple imputation using chained equations (MICE) is the most popular method for imputation in social science research. Its popularity is based primarily on its ability to impute multiple variables of different types simultaneously and conditional on one another.</p>
<p>One iteration of the imputation algorithm has the following general logic:</p>
<p><span class="math display">\[\begin{equation}
\begin{array}{clc}

X_{1} \leftarrow \boldsymbol{X_{-1}}, Z \\
X_{2} \leftarrow \boldsymbol{X_{-2}}, Z \\
X_{3} \leftarrow \boldsymbol{X_{-3}}, Z

\end{array}
\end{equation}\]</span></p>
<p>In essence, MICE is based on a series of univariate imputation models, each of which is selected to correspond to the type of variable ( e.g. continuous, categorical, proportion, etc.) that is being imputed. It uses chained equations, meaning that an outcome in one imputation equation by default (and this can be customized) is a predictor in all other equations. MICE is similar in logic to MCMC in that it builds a chain, iterates until the chain attains its stationary distribution and samples from it to replace the missing observations.</p>
<p>Let’s apply this method to items 1 of scales 1 and 4:</p>

<pre class='stata'>. mi impute chained (regress) `s4' (ologit, augment) `s1' = x1* x2* x3* y*, ///
>         add(5) burnin(100) chaindots savetrace(mytrace, replace)

Conditional models:
         s4_i1_tp3: regress s4_i1_tp3 i.s1_i1_tp2 s4_i1_tp2 i.s1_i1_tp3 i.s1_i1_tp1 s4_i1_tp1 x11
                     x12 x13 x2 x31 x32 x33 y1 y2 y3
         s1_i1_tp2: ologit s1_i1_tp2 s4_i1_tp3 s4_i1_tp2 i.s1_i1_tp3 i.s1_i1_tp1 s4_i1_tp1 x11 x12
                     x13 x2 x31 x32 x33 y1 y2 y3 , augment
         s4_i1_tp2: regress s4_i1_tp2 s4_i1_tp3 i.s1_i1_tp2 i.s1_i1_tp3 i.s1_i1_tp1 s4_i1_tp1 x11
                     x12 x13 x2 x31 x32 x33 y1 y2 y3
         s1_i1_tp3: ologit s1_i1_tp3 s4_i1_tp3 i.s1_i1_tp2 s4_i1_tp2 i.s1_i1_tp1 s4_i1_tp1 x11 x12
                     x13 x2 x31 x32 x33 y1 y2 y3 , augment
         s1_i1_tp1: ologit s1_i1_tp1 s4_i1_tp3 i.s1_i1_tp2 s4_i1_tp2 i.s1_i1_tp3 s4_i1_tp1 x11 x12
                     x13 x2 x31 x32 x33 y1 y2 y3 , augment
         s4_i1_tp1: regress s4_i1_tp1 s4_i1_tp3 i.s1_i1_tp2 s4_i1_tp2 i.s1_i1_tp3 i.s1_i1_tp1 x11
                     x12 x13 x2 x31 x32 x33 y1 y2 y3

Performing chained iterations:
  imputing m=1: burn-in 100 .........10.........20.........30.........40.........50.........60........
> .70.........80.........90.........100 done
  imputing m=2: burn-in 100 .........10.........20.........30.........40.........50.........60........
> .70.........80.........90.........100 done
  imputing m=3: burn-in 100 .........10.........20.........30.........40.........50.........60........
> .70.........80.........90.........100 done
  imputing m=4: burn-in 100 .........10.........20.........30.........40.........50.........60........
> .70.........80.........90.........100 done
  imputing m=5: burn-in 100 .........10.........20.........30.........40.........50.........60........
> .70.........80.........90.........100 done

Multivariate imputation                     Imputations =        5
Chained equations                                 added =        5
Imputed: m=1 through m=5                        updated =        0

Initialization: monotone                     Iterations =      500
                                                burn-in =      100

         s4_i1_tp1: linear regression
         s4_i1_tp2: linear regression
         s4_i1_tp3: linear regression
         s1_i1_tp1: ordered logistic regression
         s1_i1_tp2: ordered logistic regression
         s1_i1_tp3: ordered logistic regression

───────────────────┬──────────────────────────────────────────────
                   │               Observations per m             
                   ├───────────────────────────────────┬──────────
          Variable │   Complete   Incomplete   Imputed │     Total
───────────────────┼───────────────────────────────────┼──────────
         s4_i1_tp1 │        185           15        15 │       200
         s4_i1_tp2 │        190           10        10 │       200
         s4_i1_tp3 │        193            7         7 │       200
         s1_i1_tp1 │        188           12        12 │       200
         s1_i1_tp2 │        192            8         8 │       200
         s1_i1_tp3 │        190           10        10 │       200
───────────────────┴───────────────────────────────────┴──────────
(complete + incomplete = total; imputed is the minimum across m
 of the number of filled-in observations.)
</pre>
<p>Diagnostics:</p>

<p><img src="chained.png" /></p>
<p>Autocorrelation plot for the first three chains:</p>
<p><img src="cchained_s1_i1_tp1_ac.png" /></p>
<h2 id="using-imputed-datasets-plus-some-additional-diagnostics">Using imputed datasets (plus some additional diagnostics)</h2>
<p>We use the imputed datasets to produce estimates of quantities of interest. To do that, we use Stata’ command <code>mi estimate</code>.</p>
<p>For instance, suppose we wanted to estimate the means of s1 across time. We would do:</p>
<pre class='stata'>. qui {

. mi estimate: mean s1_i1, over(time)

Multiple-imputation estimates     Imputations     =          5
Mean estimation                   Number of obs   =        600
                                  Average RVI     =     0.0117
                                  Largest FMI     =     0.0153
                                  Complete DF     =        599
DF adjustment:   Small sample     DF:     min     =     568.93
                                          avg     =     578.22
Within VCE type:     Analytic             max     =     585.87

            1: time = 1
            2: time = 2
            3: time = 3

─────────────┬────────────────────────────────────────────────
        Over │       Mean   Std. Err.     [95% Conf. Interval]
─────────────┼────────────────────────────────────────────────
           1 │       .838   .0676022      .7052197    .9707803
           2 │       .858   .0681253      .7242004    .9917996
           3 │       .784   .0678255      .6507864    .9172136
─────────────┴────────────────────────────────────────────────
</pre>
<p>Or we can also run a regression:</p>
<pre class='stata'>. mi estimate: mixed y s1_i1 x1 x2 x3 || id:

Multiple-imputation estimates                   Imputations       =          5
Mixed-effects ML regression                     Number of obs     =        600

Group variable: id                              Number of groups  =        200
                                                Obs per group:
                                                              min =          3
                                                              avg =        3.0
                                                              max =          3
                                                Average RVI       =     0.0038
                                                Largest FMI       =     0.0089
DF adjustment:   Large sample                   DF:     min       =  51,437.76
                                                        avg       = 7211067.52
                                                        max       =   2.90e+07
Model F test:       Equal FMI                   F(   4,610224.4)  =    1905.13
                                                Prob > F          =     0.0000

─────────────┬────────────────────────────────────────────────────────────────
           y │      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
─────────────┼────────────────────────────────────────────────────────────────
    s1_i1_tp │   .3859707   .0625264     6.17   0.000     .2634183    .5085232
          x1 │   1.440248   .0276643    52.06   0.000     1.386027    1.494469
          x2 │  -.2534587   .0369531    -6.86   0.000    -.3258854   -.1810319
          x3 │   1.744699   .0368421    47.36   0.000     1.672489    1.816908
       _cons │  -.7931172   .2508366    -3.16   0.002    -1.284748   -.3014865
─────────────┴────────────────────────────────────────────────────────────────

─────────────────────────────┬────────────────────────────────────────────────
  Random-effects Parameters  │   Estimate   Std. Err.     [95% Conf. Interval]
─────────────────────────────┼────────────────────────────────────────────────
id: Identity                 │
                   sd(_cons) │   .3642743   .0782776      .2390632    .5550657
─────────────────────────────┼────────────────────────────────────────────────
                sd(Residual) │   1.026121   .0368901      .9563057    1.101032
─────────────────────────────┴────────────────────────────────────────────────
</pre>
<p>Additional diagnostics can be called after <code>mi estimate</code>.</p>
<pre class='stata'>. mi estimate, vartable dftable
note: dftable or vartable imply estmetric

Multiple-imputation estimates                   Imputations       =          5
Mixed-effects ML regression

Variance information
─────────────┬────────────────────────────────────────────────────────────────
             │        Imputation variance                             Relative
             │    Within   Between     Total       RVI       FMI    efficiency
─────────────┼────────────────────────────────────────────────────────────────
y            │
    s1_i1_tp │   .003875   .000029    .00391   .008897   .008857       .998232
          x1 │   .000764   1.3e-06   .000765   .001999   .001997       .999601
          x2 │   .001365   5.1e-07   .001366   .000448   .000448        .99991
          x3 │    .00135   5.7e-06   .001357     .0051   .005087       .998984
       _cons │   .062896   .000019   .062919   .000372   .000372       .999926
─────────────┼────────────────────────────────────────────────────────────────
lns1_1_1     │
       _cons │   .045773   .000336   .046176   .008802   .008763        .99825
─────────────┼────────────────────────────────────────────────────────────────
lnsig_e      │
       _cons │   .001288   4.0e-06   .001292   .003732   .003725       .999256
─────────────┴────────────────────────────────────────────────────────────────


Multiple-imputation estimates                   Imputations       =          5
Mixed-effects ML regression                     Number of obs     =        600

Group variable: id                              Number of groups  =        200
                                                Obs per group:
                                                              min =          3
                                                              avg =        3.0
                                                              max =          3
                                                Average RVI       =     0.0038
                                                Largest FMI       =     0.0089
DF adjustment:   Large sample                   DF:     min       =  51,437.76
                                                        avg       = 7211067.52
                                                        max       =   2.90e+07
Model F test:       Equal FMI                   F(   4,610224.4)  =    1905.13
                                                Prob > F          =     0.0000

─────────────┬────────────────────────────────────────────────────────────────
             │                                                      % Increase
           y │      Coef.   Std. Err.      t    P>|t|           DF   Std. Err.
─────────────┼────────────────────────────────────────────────────────────────
y            │
    s1_i1_tp │   .3859707   .0625264     6.17   0.000      51437.8        0.44
          x1 │   1.440248   .0276643    52.06   0.000    1004709.8        0.10
          x2 │  -.2534587   .0369531    -6.86   0.000      2.0e+07        0.02
          x3 │   1.744699   .0368421    47.36   0.000     155375.4        0.25
       _cons │  -.7931172   .2508366    -3.16   0.002      2.9e+07        0.02
─────────────┼────────────────────────────────────────────────────────────────
lns1_1_1     │
       _cons │  -1.009848   .2148864                       52543.3        0.44
─────────────┼────────────────────────────────────────────────────────────────
lnsig_e      │
       _cons │   .0257853    .035951                      289404.4        0.19
─────────────┴────────────────────────────────────────────────────────────────
</pre>
<p>Within, between and total variance show the magnitude of the respective quantities in the Rubin’s rules.</p>
<p>Relative Variance Increase (RVI) shows the proportional increase in variance of an estimate due to the missingness in the data (compared to data without missigness).</p>
<p>Fraction of Missing Information (FMI) shows the propotion of total variance that is due to missingness.</p>
<p>Relative Efficiency (RE) shows how well the true parameters are estimated compared to the estimates obtained from an infinite number of imputations.</p>
<hr />
<p><em>Some ‘ground rules’ of imputation</em></p>
<p>Multiple imputation is an extremely useful technique that can help us tackle missing data problems in a principled and statistically sound manner. However, practitioners should remain vigilent and not fall into some common pitfalls associated with the procedure.</p>
<p>To avoid trouble:</p>
<ul>
<li>We need to understand our data very well. We need to know whether all conditions for imputation are met, variables it makes sense to impute and variables that we should not impute for certain groups (e.g. wages for one-year-olds).</li>
<li>We should not rely on statistical software to know how to handle our data properly. Indiscriminately throwing variables into an imputation routine and expecting the software to know their meaning and make decisions for us is a recipe for disaster. Likewise, cherry picking variables to include in imputation models is nothing short of bad research practice.</li>
<li>We must pay attention to attrition and selection. Imputation algorithms do not know anything about this aspect of our missing data problem and would assume it away. Borrowing information from non-comparable subjects could severely affect the quality of our imputation and inference.</li>
<li>The amount of missingness and sample size matter for the performance of imputation routines! The rule of thumb is that missingness of about 10% is manageable, and missingness of 25% is high and likely problematic, especially in samples of smaller size.</li>
</ul>
<hr />
<h2 id="references">References</h2>
<p>Mack C, Su Z, Westreich D. Rockville (MD): Agency for Healthcare Research and Quality (US); 2018 Feb.</p>
<p>Little, R. J. A. 1988. A test of missing completely at random for multivariate data with missing values. Journal of the American Statistical Association 83: 1198–1202.</p>
<p>Cheng Li, Little’s test of missing completely at random The Stata Journal (2013) 13, Number 4, pp. 795–809</p>
<p>Rubin, Multiple Imputation After 18+ Years,Journal of the American Statistical Association, Vol. 91, No. 434 (Jun., 1996), pp.473-489</p>
<p>Lumley, T., 2010, Complex Surveys. A guide to analysis in R.</p>
<p>van Buuren, S., Groothuis-Oudshoorn, K., 2011, mice: Multivariate Imputation by Chained Equations in R, Journal of Statistical Software</p>
<p>Stata, 2017, Stata Multiple-Imputation Reference Manual, Stata Crop LLC, College Station, Texas</p>
<p>UCLA IDRE, Multiple Imputation in Stata, https://stats.idre.ucla.edu/stata/seminars/mi_in_stata_pt1_new/</p>
</body>
</html>
